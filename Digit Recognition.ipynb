{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.ToTensor()\n",
    "\n",
    "train_data=datasets.MNIST(root='../MNIST_data',download=True,train=True,transform=transform)\n",
    "test_data=datasets.MNIST(root='../MNIST_data',download=True,train=False,transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data.sampler import SubsetRandomSampler,random_split\n",
    "# valid_size = 0.2\n",
    "\n",
    "# # obtain training indices that will be used for validation\n",
    "# num_train = len(train_data)\n",
    "# print(num_train)\n",
    "# indices = list(range(num_train))\n",
    "# np.random.shuffle(indices)\n",
    "# split = int(np.floor(valid_size * num_train))\n",
    "# print(split)\n",
    "# train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# # define samplers for obtaining training and validation batches\n",
    "# #train_sampler = SubsetRandomSampler(train_idx)\n",
    "# #valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SubsetRandomSampler,random_split\n",
    "train_set,validation_set=random_split(train_data,[int(0.8*len(train_data)),int(0.2*len(train_data))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size=20\n",
    "train_loader=DataLoader(train_set,batch_size=batch_size)\n",
    "valid_loader=DataLoader(validation_set,batch_size=batch_size)\n",
    "test_loader=DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.2)\n",
      "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.2)\n",
      "  (6): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "model=nn.Sequential(\n",
    "                      nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(64, 10),\n",
    "                   )\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.135720 \tValidation Loss: 0.429306\n",
      "Validation loss decreased (inf --> 0.429306).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.445268 \tValidation Loss: 0.313916\n",
      "Validation loss decreased (0.429306 --> 0.313916).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.348292 \tValidation Loss: 0.259261\n",
      "Validation loss decreased (0.313916 --> 0.259261).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.290256 \tValidation Loss: 0.221010\n",
      "Validation loss decreased (0.259261 --> 0.221010).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.255953 \tValidation Loss: 0.193248\n",
      "Validation loss decreased (0.221010 --> 0.193248).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.224580 \tValidation Loss: 0.173427\n",
      "Validation loss decreased (0.193248 --> 0.173427).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.201901 \tValidation Loss: 0.157909\n",
      "Validation loss decreased (0.173427 --> 0.157909).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.183551 \tValidation Loss: 0.144871\n",
      "Validation loss decreased (0.157909 --> 0.144871).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.169628 \tValidation Loss: 0.135724\n",
      "Validation loss decreased (0.144871 --> 0.135724).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.158106 \tValidation Loss: 0.127334\n",
      "Validation loss decreased (0.135724 --> 0.127334).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.148335 \tValidation Loss: 0.121225\n",
      "Validation loss decreased (0.127334 --> 0.121225).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.138649 \tValidation Loss: 0.114933\n",
      "Validation loss decreased (0.121225 --> 0.114933).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.131200 \tValidation Loss: 0.111340\n",
      "Validation loss decreased (0.114933 --> 0.111340).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.125743 \tValidation Loss: 0.105858\n",
      "Validation loss decreased (0.111340 --> 0.105858).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.116687 \tValidation Loss: 0.103734\n",
      "Validation loss decreased (0.105858 --> 0.103734).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.112903 \tValidation Loss: 0.101555\n",
      "Validation loss decreased (0.103734 --> 0.101555).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.105906 \tValidation Loss: 0.097894\n",
      "Validation loss decreased (0.101555 --> 0.097894).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.102875 \tValidation Loss: 0.094600\n",
      "Validation loss decreased (0.097894 --> 0.094600).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.098204 \tValidation Loss: 0.092289\n",
      "Validation loss decreased (0.094600 --> 0.092289).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.096835 \tValidation Loss: 0.090650\n",
      "Validation loss decreased (0.092289 --> 0.090650).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.092490 \tValidation Loss: 0.089995\n",
      "Validation loss decreased (0.090650 --> 0.089995).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.089686 \tValidation Loss: 0.088909\n",
      "Validation loss decreased (0.089995 --> 0.088909).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.084678 \tValidation Loss: 0.086267\n",
      "Validation loss decreased (0.088909 --> 0.086267).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.082503 \tValidation Loss: 0.086487\n",
      "Epoch: 25 \tTraining Loss: 0.079951 \tValidation Loss: 0.085512\n",
      "Validation loss decreased (0.086267 --> 0.085512).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.076824 \tValidation Loss: 0.084122\n",
      "Validation loss decreased (0.085512 --> 0.084122).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.075722 \tValidation Loss: 0.083676\n",
      "Validation loss decreased (0.084122 --> 0.083676).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.073489 \tValidation Loss: 0.083190\n",
      "Validation loss decreased (0.083676 --> 0.083190).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.072192 \tValidation Loss: 0.081110\n",
      "Validation loss decreased (0.083190 --> 0.081110).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.068731 \tValidation Loss: 0.081253\n",
      "Epoch: 31 \tTraining Loss: 0.066530 \tValidation Loss: 0.079386\n",
      "Validation loss decreased (0.081110 --> 0.079386).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.064952 \tValidation Loss: 0.080023\n",
      "Epoch: 33 \tTraining Loss: 0.065656 \tValidation Loss: 0.080521\n",
      "Epoch: 34 \tTraining Loss: 0.062681 \tValidation Loss: 0.079397\n",
      "Epoch: 35 \tTraining Loss: 0.061070 \tValidation Loss: 0.078801\n",
      "Validation loss decreased (0.079386 --> 0.078801).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.059708 \tValidation Loss: 0.078372\n",
      "Validation loss decreased (0.078801 --> 0.078372).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.057979 \tValidation Loss: 0.079881\n",
      "Epoch: 38 \tTraining Loss: 0.058375 \tValidation Loss: 0.078959\n",
      "Epoch: 39 \tTraining Loss: 0.058090 \tValidation Loss: 0.076637\n",
      "Validation loss decreased (0.078372 --> 0.076637).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.054082 \tValidation Loss: 0.075910\n",
      "Validation loss decreased (0.076637 --> 0.075910).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.052516 \tValidation Loss: 0.075237\n",
      "Validation loss decreased (0.075910 --> 0.075237).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.052348 \tValidation Loss: 0.076441\n",
      "Epoch: 43 \tTraining Loss: 0.051975 \tValidation Loss: 0.075447\n",
      "Epoch: 44 \tTraining Loss: 0.050332 \tValidation Loss: 0.075567\n",
      "Epoch: 45 \tTraining Loss: 0.048533 \tValidation Loss: 0.078041\n",
      "Epoch: 46 \tTraining Loss: 0.047240 \tValidation Loss: 0.077656\n",
      "Epoch: 47 \tTraining Loss: 0.046538 \tValidation Loss: 0.075734\n",
      "Epoch: 48 \tTraining Loss: 0.046925 \tValidation Loss: 0.074861\n",
      "Validation loss decreased (0.075237 --> 0.074861).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.043892 \tValidation Loss: 0.073983\n",
      "Validation loss decreased (0.074861 --> 0.073983).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.043462 \tValidation Loss: 0.074960\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 50\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        data=data.view(data.shape[0], -1)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        data=data.view(data.shape[0], -1)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        torch.save(model, 'model_full.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.064548\n",
      "\n",
      "Test Accuracy of     0: 99% (971/980)\n",
      "Test Accuracy of     1: 99% (1124/1135)\n",
      "Test Accuracy of     2: 97% (1010/1032)\n",
      "Test Accuracy of     3: 98% (994/1010)\n",
      "Test Accuracy of     4: 98% (965/982)\n",
      "Test Accuracy of     5: 97% (871/892)\n",
      "Test Accuracy of     6: 97% (938/958)\n",
      "Test Accuracy of     7: 97% (1001/1028)\n",
      "Test Accuracy of     8: 97% (954/974)\n",
      "Test Accuracy of     9: 97% (985/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9813/10000)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    data=data.view(data.shape[0], -1)\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 784])\n",
      "tensor([7]) tensor(7)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image as l\n",
    "o=l.open('mod.png')\n",
    "\n",
    "#img.shape\n",
    "from torchvision import transforms\n",
    "m1=transforms.Grayscale().__call__(o)\n",
    "m=transforms.ToTensor().__call__(m1)\n",
    "\n",
    "#m=torch.from_numpy(img)\n",
    "print(m.shape)\n",
    "m=m.view(1, -1)\n",
    "print(m.shape)\n",
    "#x=torch.load('model.pt')\n",
    "\n",
    "data, target=next(iter(test_loader))\n",
    "# forward pass: compute predicted outputs by passing inputs to the model\n",
    "dat=data[0].view(1, -1)\n",
    "\n",
    "#print(x)\n",
    "with torch.no_grad():\n",
    "    output = model(dat)\n",
    "    _, pred = torch.max(output, 1)\n",
    "\n",
    "print(pred,target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
